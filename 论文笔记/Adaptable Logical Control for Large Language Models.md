# 文献阅读报告

| 基本信息      |                                                     |
| ------------- | --------------------------------------------------- |
| 文章标题      | Adaptable Logical Control for Large Language Models |
| 标题翻译      | 大型语言模型的自适应逻辑控制                        |
| 所属刊物/会议 | NeurIPS CCF-A                                       |
| 发表年份      | 2024                                                |

## 摘要

LLMs（如GPT-4）虽然能生成流畅自然的语言，但当用户希望其生成的文本必须满足某些**逻辑约束**（如包含特定关键词、限定文本长度、必须以某短语结尾等）时，现有方法常常失败。

本文介绍了 Ctrl-G，这是一个神经符号框架，能够以易于处理且适应性强的方式控制 LLM 生成，使其可靠地遵循逻辑约束。Ctrl-G 将任何可用于生产的 LLM 与隐马尔可夫模型 (HMM) 相结合，引导 LLM 输出遵循以确定性有限自动机表示的逻辑约束。我们证明，当 TULU2-7B 模型与 2B 参数 HMM 结合时，Ctrl-G 在文本编辑方面的表现优于 GPT4：在遵循逻辑约束生成文本插入/延续的任务中，我们的方法在人工评估中的满意率提高了 30% 以上。当应用于中型语言模型（例如 GPT2large）时，Ctrl-G 在标准基准测试中也大幅超越其同类产品。此外，作为概念验证研究，我们使用 Ctrl-G 来辅助 GSM 基准上的 LLM 推理，预示着 Ctrl-G 以及其他受限生成方法在传统语言生成任务之外的应用。

## 解决的问题

**如何在推理阶段高效地控制LLM以满足所需的逻辑约束**

